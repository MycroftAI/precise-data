This is the original hey-mycroft model. 

Built using code line version 0.1.

Epochs - 6,000 (but not positive, its a guess)
Batch Size = 5,000 (this I know because it is hard coded)
Drop Out = 0.2 (pretty sure)
RCUs = 20 (this I know because it is hard coded)
Sensitivity = 0.8 (This I know because of a bug I discovered)

It used the data found in the tags.txt file which represents the data found in the /mnt/nas/wakewords directory. I am not sure about the distribution of these files across training and validation.

